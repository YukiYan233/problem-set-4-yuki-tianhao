---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
PRVDR_CTGRY_SBTYP_CD - Provider Category Subtype Code
PRVDR_CTGRY_CD - Provider Category Code
CHOW_DT - Change of Ownership Date
CITY_NAME - City where the facility is located
STATE_CD - State code
SSA_CNTY_CD - County code
ZIP_CD - Zip code of the facility
CRTFD_BED_CNT - Certified bed count
FAC_NAME - Facility name
ST_ADR - Street address
PRVDR_NUM - Provider Number

2. 
    a.
 ```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Define a function to load and filter data for short-term hospitals by year
def load_and_filter_pos(file_path, year):
    # Load the data with a specified encoding
    pos_data = pd.read_csv(file_path, low_memory=False, encoding='latin1')
    
    # Filter for short-term hospitals
    short_term_hospitals = pos_data[(pos_data['PRVDR_CTGRY_CD'] == 1) & 
                                    (pos_data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    
    # Add a column for the year
    short_term_hospitals['Year'] = year
    print(f"Year {year} - Number of short-term hospitals: {len(short_term_hospitals)}")  # Check the count
    return short_term_hospitals

# Load and filter each year's dataset individually
pos2016 = load_and_filter_pos('pos2016.csv', 2016)
pos2017 = load_and_filter_pos('pos2017.csv', 2017)
pos2018 = load_and_filter_pos('pos2018.csv', 2018)
pos2019 = load_and_filter_pos('pos2019.csv', 2019)

# Append the datasets together
all_years_data = pd.concat([pos2016, pos2017, pos2018, pos2019], ignore_index=True)

# Count the number of observations by year in the combined data
observations_by_year = all_years_data['Year'].value_counts().sort_index()

# Plot the number of observations by year with more precise y-axis values
plt.figure(figsize=(10, 6))
observations_by_year.plot(kind='bar')
plt.title('Number of Short-Term Hospital Observations by Year')
plt.xlabel('Year')
plt.ylabel('Number of Observations')
plt.ylim(7240, 7305)
plt.yticks(range(7240, 7306, 5))

# Set y-axis with a smaller range of intervals to capture subtle differences
plt.yticks(range(min(observations_by_year) - 10, max(observations_by_year) + 10, 10))

plt.xticks(rotation=0)
plt.show()

```

4. 
    a.
```{python}
import pandas as pd
import matplotlib.pyplot as plt

unique_hospitals_by_year = all_years_data.groupby('Year')['PRVDR_NUM'].nunique()

# Plot the number of unique hospitals per year
plt.figure(figsize=(10, 6))
unique_hospitals_by_year.plot(kind='bar')
plt.title('Number of Unique Short-Term Hospitals by Year')
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.ylim(7240, 7305)  # Set a similar y-axis range for easy comparison
plt.xticks(rotation=0)
plt.show()

# Display the count for reference
print(unique_hospitals_by_year)

```

    b.
    Each CMS certification number (PRVDR_NUM) appears only once per year. This indicates that the dataset is structured so that each hospital has a single, unique record in each year, without duplicate entries or multiple records reflecting different operational statuses or characteristics within the same year. Since there are no duplicates within each year, the dataset is reliable for examining trends over time, such as changes in the number of active short-term hospitals each year. The increase in counts from 2016 to 2019 reflects an actual trend rather than potential data artifacts caused by duplicated records.

## Identify hospital closures in POS file (15 pts) (*)

1. 
2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    The .shp is Shapefile, which contains the geometry (spatial features) of the shapes, such as points, lines, or polygons. This has 845.9 MB which is largest. The .shx (Shape Index File), which is an index file that allows for quick access to the geometry in the .shp file. It essentially points to the spatial features, making it faster to query the shapefile. It has 266kb. The .dbf (Database File) contains attribute data (tabular data) related to each shape, stored in dBase format. Attributes might include details like names, zip codes, or other metadata associated with each geographic feature. It has 6.4 MB. The .prj (Projection File) defines the coordinate system and map projection used in the shapefile. This has 4 KB. The .xml (Metadata File) contains metadata about the shapefile, such as data source, creation information, and attribute descriptions. It has 16 KB.
2. 
```{python}
import geopandas as gpd

```

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
