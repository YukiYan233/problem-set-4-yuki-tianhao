---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
PRVDR_CTGRY_SBTYP_CD - Provider Category Subtype Code
PRVDR_CTGRY_CD - Provider Category Code
PRVDR_NUM - CMS Certification Number
PGM_TRMNTN_CD - current termination status
CITY_NAME - City where the facility is located
STATE_CD - State code
ZIP_CD - Zip code of the facility
FAC_NAME - Facility name
ST_ADR - Street address

2. 
    a.
 ```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Define a function to load and filter data for short-term hospitals by year
def load_and_filter_pos(file_path, year):
    # Load the data with a specified encoding
    pos_data = pd.read_csv(file_path, low_memory=False, encoding='latin1')
    
    # Filter for short-term hospitals
    short_term_hospitals = pos_data[(pos_data['PRVDR_CTGRY_CD'] == 1) & 
                                    (pos_data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    
    # Add a column for the year
    short_term_hospitals['Year'] = year
    print(f"Year {year} - Number of short-term hospitals: {len(short_term_hospitals)}")  # Check the count
    return short_term_hospitals

# Load and filter each year's dataset individually
pos2016 = load_and_filter_pos('pos2016.csv', 2016)
pos2017 = load_and_filter_pos('pos2017.csv', 2017)
pos2018 = load_and_filter_pos('pos2018.csv', 2018)
pos2019 = load_and_filter_pos('pos2019.csv', 2019)

# Append the datasets together
all_years_data = pd.concat([pos2016, pos2017, pos2018, pos2019], ignore_index=True)

# Count the number of observations by year in the combined data
observations_by_year = all_years_data['Year'].value_counts().sort_index()

# Plot the number of observations by year with more precise y-axis values
plt.figure(figsize=(10, 6))
observations_by_year.plot(kind='bar')
plt.title('Number of Short-Term Hospital Observations by Year')
plt.xlabel('Year')
plt.ylabel('Number of Observations')
plt.ylim(7240, 7305)
plt.yticks(range(7240, 7306, 5))

# Set y-axis with a smaller range of intervals to capture subtle differences
plt.yticks(range(min(observations_by_year) - 10, max(observations_by_year) + 10, 10))

plt.xticks(rotation=0)
plt.show()

```

4. 
    a.
```{python}
import pandas as pd
import matplotlib.pyplot as plt

unique_hospitals_by_year = all_years_data.groupby('Year')['PRVDR_NUM'].nunique()

# Plot the number of unique hospitals per year
plt.figure(figsize=(10, 6))
unique_hospitals_by_year.plot(kind='bar')
plt.title('Number of Unique Short-Term Hospitals by Year')
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.ylim(7240, 7305)  # Set a similar y-axis range for easy comparison
plt.xticks(rotation=0)
plt.show()

# Display the count for reference
print(unique_hospitals_by_year)

```

    b.
    Each CMS certification number (PRVDR_NUM) appears only once per year. This indicates that the dataset is structured so that each hospital has a single, unique record in each year, without duplicate entries or multiple records reflecting different operational statuses or characteristics within the same year. Since there are no duplicates within each year, the dataset is reliable for examining trends over time, such as changes in the number of active short-term hospitals each year. The increase in counts from 2016 to 2019 reflects an actual trend rather than potential data artifacts caused by duplicated records.

## Identify hospital closures in POS file (15 pts) (*)

1. 
```{python}
def load_and_filter_pos(file_path, year):
    pos_data = pd.read_csv(file_path, low_memory=False, encoding='latin1')
    pos_data['Year'] = year
    return pos_data

pos2016 = load_and_filter_pos('pos2016.csv', 2016)
pos2017 = load_and_filter_pos('pos2017.csv', 2017)
pos2018 = load_and_filter_pos('pos2018.csv', 2018)
pos2019 = load_and_filter_pos('pos2019.csv', 2019)
all_years_data = pd.concat([pos2016, pos2017, pos2018, pos2019], ignore_index=True)
```
```{python}
pivoted_data = all_years_data.pivot_table(
    index=['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD'], 
    columns='Year', 
    values='PGM_TRMNTN_CD', 
    aggfunc='first'
).reset_index()
```
```{python}
pivoted_data['Suspected_Closure_Year'] = pivoted_data.apply(
    lambda row: next((year for year in [2017, 2018, 2019] if \
        (pd.isna(row[year]) or row[year] != 0) and row[2016] == 0), None), axis=1)
suspected_closures = pivoted_data.dropna(subset=['Suspected_Closure_Year'])
```
```{python}
suspected_closures_list = suspected_closures[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']]
total_suspected_closures = len(suspected_closures)
print(total_suspected_closures)
```
15730 hospital fit this definition
2. 
```{python}
sorted_suspected_closures = suspected_closures_list.sort_values(by='FAC_NAME')
sorted_suspected_closures.head(10)
```
3. 
    a.
    ```{python}
    zip_year_counts = all_years_data[all_years_data['PGM_TRMNTN_CD'] == 0].groupby(['ZIP_CD', 'Year']).size()
    ```
    ```{python}
    suspected_closures['Is_Merger'] = suspected_closures.apply(
    lambda row: (
        zip_year_counts.get((row['ZIP_CD'], row['Suspected_Closure_Year'] + 1), 0) >= 
        zip_year_counts.get((row['ZIP_CD'], row['Suspected_Closure_Year']), 0)
    ) if row['Suspected_Closure_Year'] < 2019 else False,
    axis=1)

    potential_mergers = suspected_closures[suspected_closures['Is_Merger']]
    corrected_closures = suspected_closures[~suspected_closures['Is_Merger']]
    ```
    b.
    ```{python}
    num_potential_mergers = len(potential_mergers)
    num_corrected_closures = len(corrected_closures)
    print("Number of hospitals identified as potential mergers:", num_potential_mergers)
    print("Number of hospitals remaining after correction:", num_corrected_closures)
    ```
    c.
    ```{python}
    corrected_closures_sorted = corrected_closures[['FAC_NAME', 'ZIP_CD', 'Suspected_Closure_Year']].sort_values(by='FAC_NAME')
    corrected_closures_sorted.head(10)
    ```

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    b. 
2. 

## Calculate zip codeâ€™s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 
